{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test of scikit-learn Classifiers on iris Dataset\n",
    "We loop all the useful scikit-learn classifiers here for the iris dataset to see how they work. We first load the iris dataset from `sklearn.datasets` calling `load_irs()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X data number: 150\n",
      "X dimension number: 4\n",
      "Y cat [0, 1, 2]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import sklearn.datasets as skds\n",
    "data = skds.load_iris()\n",
    "\n",
    "x = np.array( data.data )\n",
    "y = np.array( data.target )\n",
    "target_names = data.target_names\n",
    "nx, ndim = x.shape\n",
    "print( 'X data number: %d' % nx )\n",
    "print( 'X dimension number: %d' % ndim )\n",
    "print( 'Y cat '+ str( [i for i in set(y)] ) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Iris data are continuous for all features of $X$ but has 3-categorical target $Y$. There are no missing values in the dataset so we have no data cleaning work. The first step is to split it into training and test dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sklearn.cross_validation as skcv\n",
    "\n",
    "xtrain, xtest, ytrain, ytest = skcv.train_test_split( x, y, test_size=0.33, random_state=0)\n",
    "\n",
    "def output(clf, xtrain, ytrain, xtest, ytest):\n",
    "    print( 'Train Score:%6.4f Test Score:%6.4f' % (clf.score(xtrain, ytrain), clf.score(xtest, ytest)) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## First Round with default settings\n",
    "Next we can build model for the training dataset. Since the target is categorical, we need to choose classifier type of model for this task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Score:0.8200 Test Score:0.7000\n"
     ]
    }
   ],
   "source": [
    "import sklearn.naive_bayes as sknb\n",
    "clf = sknb.MultinomialNB( )\n",
    "# fit_prior=True\n",
    "clf = clf.fit(xtrain, ytrain)\n",
    "output(clf, xtrain, ytrain, xtest, ytest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Score:0.9600 Test Score:0.9600\n"
     ]
    }
   ],
   "source": [
    "import sklearn.naive_bayes as sknb\n",
    "clf = clf = sknb.GaussianNB()\n",
    "# fit_prior=True\n",
    "clf = clf.fit(xtrain, ytrain)\n",
    "output(clf, xtrain, ytrain, xtest, ytest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Score:0.9500 Test Score:0.9000\n"
     ]
    }
   ],
   "source": [
    "import sklearn.linear_model as sklm\n",
    "clf = sklm.LogisticRegression()\n",
    "# penalty='l1'/'l2', C=1.0\n",
    "# solver='newton-cg'/'lbfgs'/'liblinear'/'sag', tol=\n",
    "clf = clf.fit(xtrain, ytrain)\n",
    "output(clf, xtrain, ytrain, xtest, ytest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Score:1.0000 Test Score:0.9600\n"
     ]
    }
   ],
   "source": [
    "import sklearn.tree as sktree\n",
    "clf = sktree.DecisionTreeClassifier()\n",
    "# max_depth=None, min_samples_split=2, random_state=0\n",
    "clf = clf.fit(xtrain, ytrain)\n",
    "output(clf, xtrain, ytrain, xtest, ytest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Score:1.0000 Test Score:0.9600\n"
     ]
    }
   ],
   "source": [
    "import sklearn.ensemble as skens\n",
    "clf = skens.BaggingClassifier()\n",
    "# base_estimator=default is DecisionTreeClassifier\n",
    "# n_estimators=10, max_samples=1.0/0.5/10, max_features=1.0/0.5/10\n",
    "# n_jobs=1/n/-1\n",
    "clf = clf.fit(xtrain, ytrain)\n",
    "output(clf, xtrain, ytrain, xtest, ytest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Score:0.9600 Test Score:0.9000\n"
     ]
    }
   ],
   "source": [
    "import sklearn.ensemble as skens\n",
    "clf = skens.AdaBoostClassifier()\n",
    "# base_estimator = default is DecisionTreeClassifier\n",
    "# n_estimators=50\n",
    "clf = clf.fit(xtrain, ytrain)\n",
    "output(clf, xtrain, ytrain, xtest, ytest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Score:1.0000 Test Score:0.9600\n"
     ]
    }
   ],
   "source": [
    "import sklearn.ensemble as skens\n",
    "clf = skens.RandomForestClassifier(n_estimators=200)\n",
    "# n_estimators=10, max_features=\"auto\"/\"sqrt\"/\"log2\"/n/None\n",
    "# max_depth=None, min_samples_split=2, bootstrap=True, random_state=None\n",
    "# n_jobs=1/n/-1\n",
    "clf = clf.fit(xtrain, ytrain)\n",
    "output(clf, xtrain, ytrain, xtest, ytest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Score:1.0000 Test Score:0.9600\n"
     ]
    }
   ],
   "source": [
    "import sklearn.ensemble as skens\n",
    "clf = skens.ExtraTreesClassifier(n_estimators=100)\n",
    "# n_estimators=10, max_depth=None, min_samples_split=1, random_state=0\n",
    "# n_jobs=1/n/-1\n",
    "clf = clf.fit(xtrain, ytrain)\n",
    "output(clf, xtrain, ytrain, xtest, ytest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Score:1.0000 Test Score:0.9600\n"
     ]
    }
   ],
   "source": [
    "import sklearn.ensemble as skens\n",
    "clf = skens.GradientBoostingClassifier( )\n",
    "# n_estimators=10, max_depth=3, min_samples_split=2, random_state=0\n",
    "# max_features=\"auto\"/\"sqrt\"/\"log2\"/n/None\n",
    "clf = clf.fit(xtrain, ytrain)\n",
    "output(clf, xtrain, ytrain, xtest, ytest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Score:0.9800 Test Score:0.9600\n"
     ]
    }
   ],
   "source": [
    "import sklearn.ensemble as skens\n",
    "import sklearn.tree as sktree\n",
    "import sklearn.naive_bayes as sknb\n",
    "clf = skens.VotingClassifier( estimators=\\\n",
    "    [('lr', sklm.LogisticRegression() ),\\\n",
    "    ( 'rf', skens.RandomForestClassifier() ),\\\n",
    "    ( 'gnb', sknb.GaussianNB())], voting='hard' )\n",
    "# estimators=[ ('clf1', clf1 ), ('clf2', clf2),... ]\n",
    "# voting='hard'/'soft'\n",
    "# weights=[w1,w2,w3]\n",
    "clf = clf.fit(xtrain, ytrain)\n",
    "output(clf, xtrain, ytrain, xtest, ytest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Score:0.9800 Test Score:0.9800\n"
     ]
    }
   ],
   "source": [
    "import sklearn.svm as sksvm\n",
    "clf = sksvm.SVC( )\n",
    "# C=1, kernel='rbf'/'linear'/'poly'/sigmoid'\n",
    "clf = clf.fit(xtrain, ytrain)\n",
    "output(clf, xtrain, ytrain, xtest, ytest)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classifier Metrics\n",
    "Besides model accuracy, confusion matrix is also needed to see how well the model performs on each data category. `sklearn.metrics` provides a lot of metrics for model evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[34  0  0]\n",
      " [ 0 29  2]\n",
      " [ 0  0 35]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      1.00      1.00        34\n",
      "          1       1.00      0.94      0.97        31\n",
      "          2       0.95      1.00      0.97        35\n",
      "\n",
      "avg / total       0.98      0.98      0.98       100\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import sklearn.metrics as skmt\n",
    "\n",
    "import sklearn.svm as sksvm\n",
    "clf = sksvm.SVC( )\n",
    "# C=1, kernel='rbf'/'linear'/'poly'/sigmoid'\n",
    "clf = clf.fit(xtrain, ytrain)\n",
    "\n",
    "predtrain = clf.predict( xtrain )\n",
    "predtest  = clf.predict( xtest )\n",
    "\n",
    "print( skmt.confusion_matrix(ytrain, predtrain) )\n",
    "print( skmt.classification_report(ytrain, predtrain) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross Validation\n",
    "Among the most common scikit-learn classifiers with their default paramter setting, SVM gets the best test score. However, other classifiers with cross-validation hyperparameter tuning may beat SVM. Let's first take a look at how scikit-learn deals with cross validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.9143  0.9697  0.9062]\n",
      "CV score 0.9301\n",
      "Train Score:0.9700 Test Score:0.9000\n"
     ]
    }
   ],
   "source": [
    "import sklearn.cross_validation as skcv\n",
    "clf = sklm.LogisticRegression(penalty=\"l1\")\n",
    "scores = skcv.cross_val_score( clf, xtrain, ytrain)\n",
    "# cv=3 (3-fold)/ n (n-fold)\n",
    "# n_jobs=1/n/-1\n",
    "print( scores )\n",
    "print( 'CV score %6.4f' % np.mean(scores) )\n",
    "clf = clf.fit( xtrain, ytrain )\n",
    "output(clf, xtrain, ytrain, xtest, ytest)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we are going to use `GridSearchCV` from `sklearn.grid_search` to find an optimal parameter for `sklm.LogisticRegression`. We have fed C=0.5, 2., 10, 50, and 100 to `GridSearchCV` and let it chooses the best C. It returns a classifier with the best C parameter. We can see that now `sklm.LogisticRegression` gets score of 0.98, which is the same as SVM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Score:0.9700 Test Score:0.9000\n",
      "Train Score:0.9900 Test Score:0.9800\n"
     ]
    }
   ],
   "source": [
    "import sklearn.grid_search as skgs\n",
    "clf = sklm.LogisticRegression(penalty=\"l1\")\n",
    "params = {'C':[0.5, 2., 10, 50, 100] }\n",
    "gsclf = skgs.GridSearchCV( clf, param_grid=params)\n",
    "# cv=3 (3-fold)/ n (n-fold)\n",
    "# n_jobs=1/n/-1\n",
    "clf = clf.fit(xtrain, ytrain)\n",
    "gsclf = gsclf.fit(xtrain, ytrain)\n",
    "output(clf, xtrain, ytrain, xtest, ytest)\n",
    "output(gs, xtrain, ytrain, xtest, ytest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
