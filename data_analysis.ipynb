{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Pandas\n",
    "\n",
    "index is what Pandas mark record, like dimensional variable in NCL\n",
    "\n",
    "## 1D\n",
    "\n",
    "`pd.Series`\n",
    "\n",
    "create from array\n",
    "\n",
    "    sr = pd.Series([x1, x2,...,xn])\n",
    "    sr = pd.Series([x1, x2,...,xn], index=[i1, i2, ..., in])\n",
    "\n",
    "create from dict\n",
    "\n",
    "    sr = pd.Series(dict)\n",
    "    sr = pd.Series(dict, index=[key1, key2, ..., keyn]) #if index is used with dict, it will pick the data by index keys\n",
    "    refer\n",
    "    sr.index = givenindex\n",
    "\n",
    "    sr[0] #by index\n",
    "    sr['a'] #by key\n",
    "    sr.get('a') #by key to avoid missing data\n",
    "\n",
    "calculation\n",
    "\n",
    "    x1+x2 #calcualtion is based on index, data with corresponding index are calculated.\n",
    "\n",
    "name attribute\n",
    "\n",
    "    sr.std()\n",
    "    sr.hist()\n",
    "\n",
    "\n",
    "## 2D\n",
    "\n",
    "`pd.DataFrame`\n",
    "\n",
    "similar to Series but has columns\n",
    "\n",
    "create from Series dict\n",
    "\n",
    "    DataFrame( {'col1': Series(..., ...),\n",
    "                'col2': Series(..., ...), ... } )\n",
    "                \n",
    "create from array dict\n",
    "\n",
    "    DataFrame( {'col1': [1, 2, 3, 4],\n",
    "                'col2': [2, 3, 4, 5], ... } )\n",
    "\n",
    "create from array dict\n",
    "\n",
    "    DataFrame( {'col1':[1,2,3]}, index=[1,2,3] )\n",
    "\n",
    "NaN is considered missing data\n",
    "\n",
    "SELECT COLUMNS\n",
    "\n",
    "    df[ 'col1' ]\n",
    "    df[ ['col1','col2'] ]\n",
    "    df.col1 (no collide with keyword)\n",
    "\n",
    "SELECT ROWS\n",
    "\n",
    "    df.irow(ind)\n",
    "    df.x[labelvalue]\n",
    "    df.ix[indexvalue]\n",
    "    df[ind1:ind2]\n",
    "\n",
    "    df[ df.col>0 ]\n",
    "    df[ df.col>0 & df.col<5 ]\n",
    "\n",
    "##  STASTISTICS\n",
    "\n",
    "df.std()\n",
    "df['col1'].max()\n",
    "\n",
    "df.info\n",
    "df.index\n",
    "df.columns\n",
    "df.T\n",
    "df.values\n",
    "\n",
    "\n",
    "\n",
    "df.dropna(how='any')\n",
    "df.fillna(value=5)\n",
    "pd.isnull(df1)\n",
    "\n",
    "\n",
    "## SQL\n",
    "\n",
    "    df.groupby( 'A' )\n",
    "    df.groupby( ['A','B'] )\n",
    "    df.groupby( ['A','B'], as_index=False ) # return to a single index\n",
    "    or\n",
    "    df.groupby( ['A','B'] ).sum().reset_index() # return to a single index\n",
    "\n",
    "    df.groupby('A').sum()\n",
    "    df.groupby('A').size() # return one column of size\n",
    "    df.groupby('A').count() # return left columns (excluding grouping column) of size\n",
    "    df.groupby('A').aggregate( somefunc ) #apply function within group\n",
    "    df.groupby('A').groups # group tuple, index list\n",
    "    df.groupby('A').get_group('groupname')\n",
    "\n",
    "    res = {}\n",
    "    for col1group, group_data in data.groupby(\"col1\"):\n",
    "        res[col1group] = group_data.col1.mean()\n",
    "    res = {}\n",
    "    for (col1group,col2group), group_data in data.groupby( [\"col1\",\"col2\"] ):\n",
    "        res[ (col1group,col2group) ] = len(group_data)\n",
    "\n",
    "\n",
    "df1, df2, df3 same column df in different rows, concat the rows\n",
    "\n",
    "    pd.concat( [df1, df2, df3] )\n",
    "    pd.concat( [df1, df4], ignore_index=True) # get rid of old index, redo the orderring\n",
    "    pd.concat( [df1, df2, df3], keys=['df1', 'df2', 'df3'] )\n",
    "    pd.concat( [df1, df2, df3], axis=0 ) # default behavior, concat rows, matching column name\n",
    "    pd.concat( [df1, df2, df3], axis=1 ) # concat columns, matching index name\n",
    "    pd.concat( [df1, df2, df3], axis=1, join='outer' ) # default behavior, concat columns, matching index name with 'outer'\n",
    "    pd.concat( [df1, df2, df3], axis=1, join='inner' ) # default behavior, concat columns, matching index name with 'inner'\n",
    "    pd.concat( [df1, df2, df3], axis=1, join_axes=[df1.index] ) # concat columns, matching index name but keep specified index\n",
    "\n",
    "    pd.join() # based on index\n",
    "    pd.merge() # based on column\n",
    "    pd.merge( left, right, on='colname')\n",
    "    pd.merge( left, right, on=['col1','col2'] )\n",
    "    pd.merge( left, right, on=['col1'], how='inner' ) #default how, how='left', 'right', 'outer','inner'\n",
    "\n",
    "\n",
    "    df['col1'].plot()\n",
    "    plt.show()\n",
    "\n",
    "## Read\n",
    "\n",
    "pd.read_csv('foo.csv')\n",
    "pd.read_csv('foo.csv', delimiter=' ')\n",
    "pd.read_csv('foo.csv', delimiter=' ', index_col='ColName', parse_date=True)\n",
    "pd.read_csv('foo.csv', delimiter=' ', index_col='ColName', usecols=['ColName'], parse_date=True)\n",
    "\n",
    "df.to_csv('foo.csv')\n",
    "\n",
    "df.head() or df.head(10)\n",
    "df.tail() or df.tail(10)\n",
    "df.describe()\n",
    "df['col1'].unique()\n",
    "\n",
    "Data Preprocessing\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "-------------------------------------------------------------------------------------\n",
    "\n",
    "\n",
    "# Gradient Descent using Scipy\n",
    "\n",
    "scipy.optimize import minimize\n",
    "    def find_w(self, w, x, y ):\n",
    "\n",
    "        nx = x.shape[0]\n",
    "        ndim = x.shape[1]\n",
    "\n",
    "        def f( inw ):\n",
    "          j = ...  \n",
    "          return \n",
    "\n",
    "        def fprime( inw ):\n",
    "            wx, h = py1( inw )\n",
    "            gradw = np.zeros( ndim )\n",
    "            for i in range(ndim):\n",
    "                gradw[i] = 1./(2*nx)*np.dot( h-y, x[:,i] )\n",
    "            return gradw\n",
    "\n",
    "        res = minimize(f, w, method='Nelder-Mead')\n",
    "        res = minimize(f, w, method='BFGS', jac=fprime)\n",
    "\n",
    "\n",
    "# sklearn\n",
    "\n",
    "    import sklearn\n",
    "\n",
    "    from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "    sk_lrmodel = LogisticRegression(C=300, penalty=\"l1\", tol=0.01)\n",
    "    sk_lrmodel.fit( xtrain, ytrain )\n",
    "    print(\"Train Accuracy: %.4f\" % sk_lrmodel.score(xtrain, ytrain))\n",
    "\n",
    "\n",
    "\n",
    "-------------------------------------------------------------------------------------\n",
    "## xlrd\n",
    "\n",
    "    import xlrd\n",
    "\n",
    "    workboox = xlrd.open_workbook(datafile)\n",
    "    sheet = workbook.sheet_by_index(0)\n",
    "\n",
    "    sheet.ncols\n",
    "    sheet.nrows\n",
    "\n",
    "    sheet.cell_value(row, col)\n",
    "    use list comprehension to get all data\n",
    "    data = [ [sheet.cell_value(row, col) for col in range(sheet.ncols)] for row in range(sheet.nrows)]\n",
    "\n",
    "    sheet.cell_type(row, col)\n",
    "\n",
    "    exceltime = sheet.cell_value(1,0)\n",
    "    xlrd.xldata_as_tuple( exceltime, 0)\n",
    "\n",
    "    sheet.col_values( col, start_rowx=startrow, end_rowx=endrow)\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
