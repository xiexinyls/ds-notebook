{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\n",
    "\\def\\Cov{\\mathrm{Cov} }\n",
    "\\def\\Var {\\mathrm{Var} }\n",
    "\\def\\diff{\\mathrm{d} }\n",
    "\\def\\Diff{\\mathrm{D} }\n",
    "\\def\\ln{\\mathrm{ln} }\n",
    "\\def\\pt{\\partial }\n",
    "\\def\\E{\\mathrm{E}}\n",
    "$\n",
    "\n",
    "# Random Variable Properties\n",
    "\n",
    "Please refer to the rigourous reference for the probability definition. Simply put, random variables are some function from the sample space $ S $. There is a probability $ P(X) $ corresponding to a set of sample events or $ X $. For discrete random variables $ X $, we use probability mass function $ p(x) $ ($R \\rightarrow R $) to describe this relationship\n",
    "\n",
    "$$ P(X=x)=p(x) $$\n",
    "\n",
    "For contiuous random variable, the probability is only define over an interval (domain) by integrating the probability density function $ f(x) $\n",
    "\n",
    "$$ P(X\\in D)=\\int _D f(x)\\diff x $$\n",
    "\n",
    "where $ D $ is a set.\n",
    "\n",
    "## Expectation\n",
    "\n",
    "If $ X $ is continuous, the definition is\n",
    "\n",
    "$$ \\E(X)=\\int_{-\\infty}^{+\\infty}xP(x)\\diff x $$\n",
    "\n",
    "If $ X $ is discrete, the definition is\n",
    "\n",
    "$$ \\E(X)=\\sum_i p(x_i)x_i $$\n",
    "\n",
    "We usually use $ \\mu $ to represent expectation\n",
    "\n",
    "$$ \\E(X)=\\mu $$\n",
    "\n",
    "## Variance\n",
    "\n",
    "$$ \\Var (X)=\\E\\{[X-\\E(X)]^{2}\\} $$\n",
    "\n",
    "We usualy use $ \\sigma^{2} $ to represent variance\n",
    "\n",
    "$$ \\Var (X)=\\sigma^{2} $$\n",
    "\n",
    "Covariance\n",
    "\n",
    "$$ \\textrm{Cov}(X_{1},X_{2})=\\E\\{ [X_{1}-\\E(X_{1})] [X_{2}-\\E(X_{2})]\\} $$\n",
    "\n",
    "## Correlation\n",
    "\n",
    "$$\\rho_{X,Y}=\\frac{\\Cov (X,Y)}{\\sqrt{\\Var(X)\\Var(Y)}}$$\n",
    "\n",
    "## General Properties\n",
    "\n",
    "For expectation\n",
    "\n",
    "$$ \\E(aX+b)=\\int_{-\\infty}^{+\\infty}(ax+b)p(x)\\diff x=a\\E(X)+b $$\n",
    "\n",
    "For variance\n",
    "\n",
    "$$ \\begin{align*}\\Var (aX) & =\\E\\{[aX-\\E(aX)]^{2}\\}\\\\& =a^{2}\\E\\{[X-\\E(X)]^{2}\\}\\\\& =a^{2}\\sigma^{2}\\end{align*} $$\n",
    "\n",
    "$$ \\begin{align*}\\Var (X)&=\\E\\{[X-\\E(X)]^{2}\\}\\\\ & =\\E[X^{2}-2\\cdot X\\cdot \\E(X)+\\E(X)^{2}]\\\\ & =\\E[X^{2}]-2\\cdot \\E(X)\\cdot \\E(X)+\\E(X)^{2}\\\\ & =\\E[X^{2}]-\\E(X)^{2}\\\\ \\sigma^{2} & =\\E[X^{2}]-\\mu^{2}\\\\ \\E(X^{2}) & =\\sigma^{2}+\\mu^{2} \\end{align*} $$\n",
    "\n",
    "For covariance\n",
    "\n",
    "$$ \\textrm{Cov}(X,X)=\\Var (X) $$\n",
    "\n",
    "$$ \\begin{align*}\\textrm{Cov}(X_{1},X_{2}) & =\\E\\{[X_{1}-\\E(X_{1})][X_{2}-\\E(X_{2})]\\}\\\\& =\\E(X_{1}X_{2})-\\E(X_{1})\\E(X_{2})\\end{align*} $$\n",
    "\n",
    "$$ \\begin{align*}\\textrm{Cov}(X_{1},aX_{2}) & =\\E[X_{1}aX_{2})]-\\E(X_{1})\\E(aX_{2})\\\\& =a[\\E(X_{1}X_{2})-\\E(X_{1})\\E(X_{2})]\\\\& =a\\textrm{Cov}(X_{1},X_{2})\\end{align*} $$\n",
    "\n",
    "$$ \\begin{align*}\\textrm{Cov}(X_{1},aX_{2}+bX_{3}) & =\\E[X_{1}(aX_{2}+bX_{3})]-\\E(X_{1})\\E(aX_{2}+bX_{3})\\\\& =\\E[X_{1}aX_{2}]-\\E(X_{1})\\E(aX_{2})\\\\& +\\E[X_{1}bX_{3}]-\\E(X_{1})\\E(bX_{3})\\\\& =a\\textrm{Cov}(X_{1},X_{2})+b\\textrm{Cov}(X_{1},X_{3})\\end{align*} $$\n",
    "\n",
    "$$ \\begin{align*}\\Var (aX_{1}+bX_{2}) & =\\textrm{Cov}(aX_{1}+bX_{2},aX_{1}+bX_{2})\\\\& =\\textrm{Cov}(aX_{1},aX_{1})+\\textrm{Cov}(aX_{1},bX_{2})\\\\& +\\textrm{Cov}(bX_{2},aX_{1})+\\textrm{Cov}(bX_{2},bX_{2})\\\\& =a^{2}\\textrm{Cov}(X_{1},X_{1})+2ab\\textrm{Cov}(X_{1},X_{2})+b^{2}\\textrm{Cov}(X_{2},X_{2})\\\\& =a^{2}\\Var (X_{1})+2ab\\textrm{Cov}(X_{1},X_{2})+b^{2}\\Var (X_{2})\\end{align*} $$\n",
    "\n",
    "## Properties for Independent Random Variables\n",
    "\n",
    "Let $ X_{1},X_{2},\\cdots,X_{n} $ be independent random variables, we have\n",
    "\n",
    "$$ \\E(X_{i}X_{j})=\\E(X_{i})\\E(X_{j}) $$\n",
    "\n",
    "$$ \\textrm{Cov}(X_{i},X_{j})=0 $$\n",
    "\n",
    "only when $ i\\neq j $. Note that when $ i=j $, the above property doesn't hold\n",
    "\n",
    "$$ \\E(X_{i}X_{j})\\neq \\E(X_{i})\\E(X_{j}) $$\n",
    "\n",
    "Other properties include\n",
    "\n",
    "$$ \\begin{align*}\\textrm{Cov}(X_{1},aX_{2}+bX_{3}) & =a\\textrm{Cov}(X_{1},X_{2})+b\\textrm{Cov}(X_{1},X_{3})=0\\end{align*} $$\n",
    "\n",
    "$$ \\Var (aX_{1}+bX_{2})=a^{2}\\Var (X_{1})+b^{2}\\Var (X_{2}) $$\n",
    "\n",
    "$$ \\begin{align*}\\Var (\\bar{X}) & =\\Var (\\frac{X_{1}}{n})+\\Var (\\frac{X_{2}}{n})+\\cdots+\\Var (\\frac{X_{n}}{n})\\end{align*} $$\n",
    "\n",
    "$$ \\begin{align*}\\Var (aX_{1}+bX_{2}) & =a^{2}\\Var (X_{1})+b^{2}\\Var (X_{2})\\\\& =[a^{2}+b^{2}]\\sigma^{2}\\end{align*} $$\n",
    "\n",
    "## Moment Generating Functions\n",
    "\n",
    "Sometimes we know the distribution of a random variable $X$ and we want to get its expectation and its variance. This is not alwasy easy. **Moment generating function** helps us get the moment of random variable easily.\n",
    "$$M(t)=\\E[e^{tX}]$$\n",
    "\n",
    "Differentiate this function by $t$ one time will give us\n",
    "$$\n",
    "\\begin{align*}\n",
    "M^{\\prime} (t) & = \\frac{\\diff }{ \\diff t } \\E [e^{tX}] \\\\\n",
    "& = \\E [ \\frac{\\diff }{ \\diff t } e^{tX} ] = \\E [X e^{tX}]\n",
    "\\end{align*}\n",
    "$$\n",
    ". Let $t=0$.\n",
    "\n",
    "$$M^{\\prime} (0)=\\E [X]$$\n",
    "\n",
    "We could have the same thing for second derivative\n",
    "\n",
    "$$M^{\\prime \\prime} (0)=\\E [X^2]$$\n",
    "\n",
    "Knowing both $\\E (X)$ and $\\E (X^2)$, we may get the variance using the relationship\n",
    "$$\\Var (X)=\\E[X^{2}]-\\E(X)^{2}$$\n",
    "\n",
    "## Distribution of Random Variable Function\n",
    "\n",
    "\n",
    "## Population and Statistics\n",
    "\n",
    "Population means a group of existing objects (finite) and potentially infinite number of objects which are generalized from experience or definition (rephrased from Wikipedia). A population is usually associated with a random variable, where the probability theory fits in. We may use a variable to represent the quantity of this population. And since this variable can take on many values and each value will have a probabiltiy associated with it. distribution is the same as random variable probability mass or probability density distribution (see probability material).\n",
    "\n",
    "If we know all the data of a **population**, we can calculate some of its statistics, which reflect its distribition, such as mean\n",
    "$$\\bar{x}=\\frac{x_1+x_2+\\cdots+x_n}{n}$$\n",
    ", median, and histogram. Standard deviation of a population is defined as\n",
    "$$SD=\\sqrt{\\frac{(x_1-\\bar{x})^2+(x_2-\\bar{x})^2+\\cdots+(x_n-\\bar{x})^2}{n}}$$\n",
    "\n",
    "## Sampling Distribution\n",
    "\n",
    "Sample random variable $X_1, X_2, \\cdots, X_n$ could be taken from the same population $X$ with same distribution $P(X=x)=f(x)$. Since we draw sample from population indepdently, the $X_1, X_2, \\cdots, X_n$ are independent and also identically distributed random. They are called **random sample**. All sample random variables together have a joint distribution $P(X_1, X_2, \\cdots, X_n)=f(x_1, x_2, \\cdots, x_n)$.\n",
    "\n",
    "Function of sample random variable (real-valued or vector-valued) $Y=T(X_1, X_2, \\cdots, X_n)$ is called a **statistic**. And the distribution of this statistic $Y$ is called **sampling distribution** of $Y$.\n",
    "\n",
    "For sample, we have several common statistics. **Sample mean** is the average of random sample\n",
    "$$ \\bar{X}=\\frac{X_{1}+X_{2}+\\cdots+X_{n}}{n} $$\n",
    "\n",
    "**Sample Variance** is defined by\n",
    "$$ S^{2}=\\frac{1}{n-1}\\sum_{i=1}^{n}(X_{i}-\\bar{X})^{2} $$\n",
    "\n",
    "**Sample Covariance** is defined between sample of $X$ and $Y$\n",
    "$$ c_{XY}=\\frac{1}{n-1}\\sum_{i=1}^{n}(X_{i}-\\bar{X})(Y_{i}-\\bar{Y}) $$\n",
    "\n",
    "**Sample Correlation** is defined between sample of $X$ and $Y$\n",
    "$$ r_{XY}=\\frac{\\sum_{i=1}^{n}(X_{i}-\\bar{X})(Y_{i}-\\bar{Y})}{\\sqrt{\\sum_{i=1}^{n}(X_{i}-\\bar{X})^{2}}\\sqrt{\\sum_{i=1}^{n}(Y_{i}-\\bar{Y})^{2}}} $$\n",
    "\n",
    "\n",
    "## Properties for Sample Statistics\n",
    "\n",
    "Since samples are from the same population and indepdent, we have $ \\E(X_{1})=\\E(X_{2})=\\cdots=\\mu $ and $ \\Var (X_{1})=\\Var (X_{2})=\\cdots=\\sigma^{2} $, and we have its sample mean $ \\bar{X}=(X_{1}+\\cdots+X_{n})/n $. We have some good properties\n",
    "\n",
    "$$ \\E (\\bar{X})=\\mu$$\n",
    "\n",
    "$$ \\begin{align*}\\Var (\\bar{X}) & =\\Var (\\frac{X_{1}}{n})+\\Var (\\frac{X_{2}}{n})+\\cdots+\\Var (\\frac{X_{n}}{n})\\\\& =n\\frac{1}{n^{2}}\\sigma^{2}=\\frac{\\sigma^{2}}{n}\\end{align*} $$\n",
    "\n",
    "$$ \\begin{align*}\\E(\\bar{X}^{2}) & =\\Var [\\bar{X}]+\\E[\\bar{X}]^{2}=\\frac{\\sigma^{2}}{n}+\\mu^{2}\\end{align*} $$\n",
    "\n",
    "It is intuitive to think that the sample variance has the follwing form\n",
    "\n",
    "$$ \\frac{1}{n}\\sum_{i}^{n}(X_{i}-\\bar{X})^{2} $$\n",
    "\n",
    ". However, we find it is actually biased. The unbiased one should be as we defined above\n",
    "\n",
    "$$ S^{2}=\\frac{1}{n-1}\\sum_{i=1}^{n}(X_{i}-\\bar{X})^{2} $$\n",
    "\n",
    "We can prove this by taking its expectation\n",
    "\n",
    "$$ \\begin{align*}\\E[\\frac{1}{n}\\sum_{i}^{n}(X_{i}-\\bar{X})^{2}] & =\\frac{1}{n}\\E[\\sum_{i}X_{i}^{2}-\\sum_{i}2X_{i}\\bar{X}+n\\bar{X}^{2}]\\\\& =\\frac{1}{n}\\E[\\sum_{i}X_{i}^{2}-2n\\bar{X}\\bar{X}+n\\bar{X}^{2}]\\\\& =\\frac{1}{n}[\\sum_{i}\\E(X_{i}^{2})-2n\\E(\\bar{X}^{2})+n\\E(\\bar{X}^{2})]\\\\& =\\frac{1}{n}[\\sum_{i}\\E(X_{i}^{2})-n\\E(\\bar{X}^{2})]\\\\& =\\frac{1}{n}[n(\\sigma^{2}+\\mu^{2})-n(\\frac{\\sigma^{2}}{n}+\\mu^{2})]\\\\& =\\frac{1}{n}(n-1)\\sigma^{2}\\neq\\sigma^{2}\\end{align*} $$\n",
    "\n",
    ", where we have used the relationship of $ \\E(X^{2}) $ and $ \\E(\\bar{X}^{2}) $.\n",
    "\n",
    "Therefore, only this form is actually unbiased.\n",
    "\n",
    "$$ \\E (S^2)=\\E ( \\frac{1}{n-1}\\sum_{i}^{n}(X_{i}-\\bar{X})^{2} )=\\sigma^2 $$\n",
    "\n",
    "# Hypothesis\n",
    "\n",
    "null hypothesis: $H_0:\\mu=\\mu_0$\n",
    "\n",
    "alternative hypothesis: $H_1:\\mu>\\mu_0, \\mu<\\mu_0, \\mu\\ne\\mu_0$\n",
    "\n",
    "Type I error: null hypothesis is true but you reject it\n",
    "Tyep II error: null hypothesis is false but you didn't reject it.\n",
    "\n",
    "\n",
    "# Reference\n",
    "\n",
    "Statistical Inference, 2002. George Casella and Roger L. Berger.\n",
    "\n",
    "A First Course in Probability eighth edition, 2010. Sheldon Ross.\n"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
