{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RL problem\n",
    "\n",
    "RL problem is closely related to Markov decision problem. We usually have a given state $S$ containing many features. And there is an action $A$ applying to this state $S$. This will change the state. The state $S$ has a probability changing into another state $S^{\\prime}$ according to the transition table $T(S,A,S^{\\prime})$. This change will produce a reward $R(S,A)$ given the previous state $S$ and action $A$, defined for our concern. The markov decision problem here is to find the optimal action $\\pi (S)$ corresponding to a given state $S$, which will give us the best reward.\n",
    "\n",
    "Several types of methods are used to solve Markov decision problem. Model based method requires all elements mentioned in the Markov decision problem. And value/model algorithms are used to solve this problem. Model free method doesn't need $T$ and $R$ function. It build a utility table. It is also called Q-Learning.\n",
    "\n",
    "## RL problem for trading\n",
    "\n",
    "When we try to solve trading problem, we want to map every element in the markov decision problem to the trading problem, so that we can solve it with a markove decision algorithm. Usually, the state $S$ in trading contains the market and holding information. Market information could be the daily or hourly return, technical and fundamental information. Holding information could be the percentage or how many shares we have for one stock or other stocks.\n",
    "\n",
    "Action $A$ usually includes buy, sell, and hold. Note that the action will change the holding information in state $S$.\n",
    "\n",
    "Reward $R$ could be simply return.\n",
    "\n",
    "We don't know $T(S,A,S^{\\prime})$ for the state transition in stock market. However, we can use simple historical estimation to get the state transition $T(S,A,S^{\\prime})$ by counting.\n",
    "\n",
    "For reward $R(S,A)$, we can simply average all reward given state $S$ and action $A$.\n",
    "\n",
    "Q table for trading is defined by the reward/utility $Q(S,A)$, which contains the sum of immediate reward and discounted reward. And the policy $\\pi (S)=\\textrm{argmax}_A (Q(S,A))$, which means the policy is the action what gives us most reward $Q(S,A)$ \n",
    "\n",
    "steps in detail\n",
    "\n",
    "1. Initialize the $Q(S,A)$ with small random number.\n",
    "2. Go through the training data. At each step, consult the $Q(S,A)$ for action and apply the best action. Observe $S^{\\prime}$ and calculate reward $R$.\n",
    "3. Update $Q(S,A)$ with the reward. Select a learning rate $\\alpha$. $Q^{\\prime}(S,A)=(1-\\alpha)Q(S,A)+\\alpha \\cdot \\textrm{improved estimate}$. The improved estimate could further be writen as the sum of immediate reward $R$ and potential later reward $\\lambda \\cdot \\textrm{later reward}$. $\\lambda$ is the discount rate. And later reward can be written as $Q[S^{\\prime},\\textrm{argmax}_{A^\\prime}(Q(S^\\prime,A^\\prime))\\,]$."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
